# Core Concepts of Deep Reinforcement Learning for Humanoids

## Introduction to Deep Reinforcement Learning (DRL)

Deep Reinforcement Learning (DRL) has emerged as a foundational paradigm for controlling **humanoid robots**, whose morphology and dynamics are inherently complex. Humanoids typically possess **high degrees of freedom (DoF)**, nonlinear dynamics, intermittent contacts (e.g., foot–ground interaction), and operate in **continuous state and action spaces**. Classical control approaches often rely on precise modeling and linearization, which become brittle or intractable as task complexity increases.

DRL addresses these challenges by combining **reinforcement learning** with **deep neural networks** to approximate policies and value functions directly from high-dimensional sensory inputs. This enables end-to-end learning of control policies that can adapt to uncertainty, exploit rich sensory feedback, and scale to whole-body behaviors such as locomotion, balancing, and manipulation.

---

## The Markov Decision Process (MDP)

At the core of DRL lies the **Markov Decision Process (MDP)**, a mathematical framework used to formalize sequential decision-making problems. An MDP is defined by five key components:

### 1. State (S)

The state represents a complete description of the system at a given time step. For humanoid robots, the state typically includes:

- Joint angles and joint velocities  
- Base position and orientation (e.g., torso pose)  
- Linear and angular velocities  
- Contact information (e.g., foot contact forces)  
- Optional sensory inputs such as IMU or vision embeddings  

Formally, at time step `t`, the system is in state:

```text
s_t ∈ S
2. Action (A)
Actions correspond to the control commands issued by the policy. In humanoid control, actions are usually continuous and may represent:

Joint torques

Desired joint positions or velocities

End-effector target forces

At each time step, the agent selects an action:

text
Copy code
a_t ∈ A
based on the current state.

3. Reward (R)
The reward function provides scalar feedback that quantifies task performance. For humanoids, rewards are carefully engineered to encourage desirable behaviors such as balance, forward motion, or energy efficiency.

text
Copy code
r_t = R(s_t, a_t)
4. Transition Probability (T)
The transition function defines the system dynamics, specifying the probability of moving to the next state given the current state and action:

text
Copy code
T(s_{t+1} | s_t, a_t)
In humanoid robotics, this transition is governed by rigid-body dynamics, contact physics, and external disturbances. While the true transition function is often unknown or intractable, DRL learns policies through interaction with a simulator or the real robot.

5. Discount Factor (γ)
The discount factor γ ∈ [0, 1) determines the importance of future rewards relative to immediate rewards. A higher γ encourages long-term stability and foresight, which is critical for humanoid tasks such as sustained walking or balancing.

The objective of DRL is to learn a policy π(a | s) that maximizes the expected discounted return:

text
Copy code
J(π) = E[ Σ_{t=0}^∞ γ^t r_t ]
Key DRL Algorithms for Humanoid Robotics
Modern humanoid control predominantly relies on policy gradient methods, which directly optimize a parameterized policy. Two widely adopted algorithms in robotics are Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC).

Proximal Policy Optimization (PPO)
PPO is an on-policy algorithm known for its simplicity and training stability. It improves upon vanilla policy gradient methods by constraining policy updates, preventing excessively large changes that could destabilize learning.

Key characteristics:

Uses a clipped surrogate objective

Relatively stable and robust across different tasks

Commonly used for humanoid locomotion and balance

Soft Actor-Critic (SAC)
SAC is an off-policy algorithm designed for high sample efficiency. It incorporates an entropy maximization objective, encouraging exploration by explicitly rewarding stochastic policies.

Key characteristics:

Learns from a replay buffer (off-policy)

Maximizes both expected return and policy entropy

More sample-efficient than on-policy methods

Core Difference: Sample Efficiency vs. Stability
PPO prioritizes training stability and ease of implementation

SAC emphasizes sample efficiency and exploration

Reward Shaping for Humanoid Control
Reward shaping is critical. Example formulation:

text
Copy code
r = w_1 * r_upright + w_2 * r_velocity - w_3 * r_energy
where w_1, w_2, w_3 are tunable weighting coefficients.

Reward shaping combines domain knowledge, empirical testing, and safety considerations.